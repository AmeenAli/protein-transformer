{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this script will be to generate a function that, essentially, can take a coordinate tensor and a mapping between those coordiates and atom identifiers (names) and creates/writes a PDB file with that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prody as pr\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "os.chdir(\"/home/jok120/protein-transformer/\")\n",
    "sys.path.append(\"/home/jok120/protein-transformer/scripts\")\n",
    "sys.path.append(\"/home/jok120/protein-transformer/scripts/utils\")\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from prody import *\n",
    "import numpy as np\n",
    "from os.path import basename, splitext\n",
    "\n",
    "import transformer.Models\n",
    "import torch.utils.data\n",
    "from dataset import ProteinDataset, paired_collate_fn, paired_collate_fn_with_len\n",
    "from protein.Structure import generate_coords_with_tuples, generate_coords\n",
    "from losses import inverse_trig_transform, copy_padding_from_gold, drmsd_loss_from_coords, mse_over_angles, combine_drmsd_mse\n",
    "from protein.Sidechains import SC_DATA, ONE_TO_THREE_LETTER_MAP, THREE_TO_ONE_LETTER_MAP\n",
    "from utils.structure_utils import onehot_to_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load a model given a checkpoint (and maybe some args.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_model(chkpt_path):\n",
    "    \"\"\" Given a checkpoint path, loads and returns the specified transformer model. Assumes \"\"\"\n",
    "    chkpt = torch.load(chkpt_path)\n",
    "    model_args = chkpt['settings']\n",
    "    model_state = chkpt['model_state_dict']\n",
    "    model_args.postnorm = False\n",
    "    print(model_args)\n",
    "    \n",
    "    the_model = transformer.Models.Transformer(model_args,\n",
    "                                               d_k=model_args.d_k,\n",
    "                                               d_v=model_args.d_v,\n",
    "                                               d_model=model_args.d_model,\n",
    "                                               d_inner=model_args.d_inner_hid,\n",
    "                                               n_layers=model_args.n_layers,\n",
    "                                               n_head=model_args.n_head,\n",
    "                                               dropout=model_args.dropout)\n",
    "    the_model.load_state_dict(model_state)\n",
    "    return the_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, buffering_mode=1, chkpt_path='./data/checkpoints/casp12_30_ln_11', clip=1.0, cluster=False, combined_loss=True, cuda=True, d_inner_hid=32, d_k=12, d_model=64, d_v=12, d_word_vec=64, data='data/proteinnet/casp12_190809_30xsmall.pt', dropout=0, early_stopping=None, epochs=40, eval_train=False, learning_rate=1e-05, log=None, log_file='./data/logs/casp12_30_ln_11.train', lr_scheduling=False, max_token_seq_len=3303, n_head=8, n_layers=6, n_warmup_steps=1000, name='casp12_30_ln_11', no_cuda=False, optimizer='adam', postnorm=False, proteinnet=True, restart=False, rnn=False, save_mode='best', train_only=False, without_angle_means=False)\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"data/checkpoints/casp12_30_ln_11_best.chkpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_data_loader(data_path, n=0, subset=\"test\"):\n",
    "    \"\"\" Given a subset of a dataset as a python dictionary file to make predictions from,\n",
    "        this function selects n items at random from that dataset to predict. It then returns a DataLoader for those\n",
    "        items, along with a list of ids.\n",
    "        \"\"\"\n",
    "    data = torch.load(data_path)\n",
    "    data_subset = data[subset]\n",
    "\n",
    "    if n is 0:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            ProteinDataset(\n",
    "                seqs=data_subset['seq'],\n",
    "                crds=data_subset['crd'],\n",
    "                angs=data_subset['ang'],\n",
    "                ),\n",
    "            num_workers=2,\n",
    "            batch_size=1,\n",
    "            collate_fn=paired_collate_fn,\n",
    "            shuffle=False)\n",
    "        return train_loader, data_subset[\"ids\"]\n",
    "\n",
    "    # We just want to predict a few examples\n",
    "    to_predict = set([s.upper() for s in np.random.choice(data_subset[\"ids\"], n)])  # [\"2NLP_D\", \"3ASK_Q\", \"1SZA_C\"]\n",
    "    will_predict = []\n",
    "    ids = []\n",
    "    seqs = []\n",
    "    angs = []\n",
    "    crds = []\n",
    "    for i, prot in enumerate(data_subset[\"ids\"]):\n",
    "        if prot.upper() in to_predict and prot.upper() not in will_predict:\n",
    "            seqs.append(data_subset[\"seq\"][i])\n",
    "            angs.append(data_subset[\"ang\"][i])\n",
    "            crds.append(data_subset[\"crd\"][i])\n",
    "            ids.append(prot)\n",
    "            will_predict.append(prot.upper())\n",
    "    assert len(seqs) == n and len(angs) == n or (len(seqs) == len(angs) and len(seqs) < n)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        ProteinDataset(\n",
    "            seqs=seqs,\n",
    "            angs=angs,\n",
    "            crds=crds),\n",
    "        num_workers=2,\n",
    "        batch_size=1,\n",
    "        collate_fn=paired_collate_fn,\n",
    "        shuffle=False)\n",
    "    return data_loader, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader, ids = get_data_loader('data/proteinnet/casp12_190809_30xsmall.pt')\n",
    "data_iter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use the model to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 129, 20])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "src_seq, src_pos_enc, tgt_ang, tgt_pos_enc, tgt_crds, tgt_crds_enc =  next(data_iter)\n",
    "print(src_seq.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 129"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(20.7834), 25.607910700575648, tensor(0.4110), tensor(1039.8550))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_ang_no_nan = tgt_ang.clone().detach()\n",
    "tgt_ang_no_nan[torch.isnan(tgt_ang_no_nan)] = 0\n",
    "pred = model.predict(src_seq, src_pos_enc)\n",
    "d_loss, d_loss_normalized, r_loss = drmsd_loss_from_coords(pred, tgt_crds, src_seq, device,\n",
    "                                                                       return_rmsd=True)\n",
    "m_loss = mse_over_angles(pred, tgt_ang).to('cpu')\n",
    "c_loss = combine_drmsd_mse(d_loss, m_loss)\n",
    "d_loss, r_loss, m_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time step: 128"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(20.7834), 25.607910700575648, tensor(0.4110), tensor(1039.8550))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_ang_no_nan = tgt_ang.clone().detach()\n",
    "tgt_ang_no_nan[torch.isnan(tgt_ang_no_nan)] = 0\n",
    "pred2 = model.forward(src_seq, src_pos_enc, tgt_ang_no_nan, tgt_pos_enc)\n",
    "d_loss, d_loss_normalized, r_loss = drmsd_loss_from_coords(pred, tgt_crds, src_seq, device,\n",
    "                                                                       return_rmsd=True)\n",
    "m_loss = mse_over_angles(pred, tgt_ang).to('cpu')\n",
    "c_loss = combine_drmsd_mse(d_loss, m_loss)\n",
    "d_loss, r_loss, m_loss, c_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7680, -0.5909, -0.8223,  ...,  0.6796,  0.0528,  0.0445],\n",
       "        [ 0.7350, -0.5812, -0.4756,  ...,  0.5694, -0.9909,  0.0753],\n",
       "        [ 0.6226, -0.8203, -0.6212,  ...,  0.3859,  0.9996, -0.0054],\n",
       "        ...,\n",
       "        [-0.3407, -0.7865,  0.1944,  ...,  0.2362,  0.9869,  0.0121],\n",
       "        [-0.3214, -0.7676,  0.3263,  ...,  0.0204,  0.9145,  0.0151],\n",
       "        [-0.3246, -0.6754,  0.3097,  ...,  0.0797,  0.6726, -0.0161]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129, 24])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 129, 24])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_ang_no_nan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = inverse_trig_transform(pred).squeeze()\n",
    "src_seq = src_seq.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = generate_coords(pred, pred.shape[0],src_seq, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1677, 3]), torch.Size([1, 1677, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords.shape, tgt_crds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANKPMQPITSTANKIVWSDPTRLSTTFSASLLRQRVKVGIAELNNVSGQYVSVYKRPAPKPEGGADAGVIMPNENQSIRTVISGSAENLATLKAEWETHKRNVDTLFASGNAGLGFLDPTAAIVSSDTT'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_letter_seq = onehot_to_seq(src_seq.squeeze().detach().numpy())\n",
    "one_letter_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDB written to 0925a_pred.pdb.\n",
      "PDB written to 0925a_true.pdb.\n"
     ]
    }
   ],
   "source": [
    "cur_map = get_13atom_mapping(one_letter_seq)\n",
    "\n",
    "title = \"0925a_pred.pdb\"\n",
    "ttitle = title.replace(\"pred\", \"true\")\n",
    "pdbc = PDB_Creator(coords.squeeze(), cur_map)\n",
    "pdbc.save_pdb(title)\n",
    "pdbc = PDB_Creator(tgt_crds.squeeze(), cur_map)\n",
    "pdbc.save_pdb(ttitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. Turn off teacher forcing for predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create a mapping from input seq to atom name list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_map_13 = {}\n",
    "for one_letter in ONE_TO_THREE_LETTER_MAP.keys():\n",
    "    atom_map_13[one_letter] = [\"N\", \"CA\", \"C\"] + list(SC_DATA[ONE_TO_THREE_LETTER_MAP[one_letter]][\"predicted\"])\n",
    "    atom_map_13[one_letter].extend([\"PAD\"]*(13-len(atom_map_13[one_letter])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_13atom_mapping(seq):\n",
    "    mapping = []\n",
    "    for residue in seq:\n",
    "        mapping.append((ONE_TO_THREE_LETTER_MAP[residue], atom_map_13[residue]))\n",
    "    return mapping\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Given a coordinate tensor and an atom mapping, create a PDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class PDB_Creator(object):\n",
    "    \"\"\"\n",
    "        A class for creating PDB files given an atom mapping.\n",
    "        The Python format string was taken from http://cupnet.net/pdb-format/.\n",
    "    \"\"\"\n",
    "    def __init__(self, coords, mapping, atoms_per_res=13):\n",
    "        self.coords = coords.detach().numpy()\n",
    "        self.mapping = mapping\n",
    "        self.atoms_per_res = atoms_per_res\n",
    "        self.format_str = \"{:6s}{:5d} {:^4s}{:1s}{:3s} {:1s}{:4d}{:1s}   {:8.3f}{:8.3f}{:8.3f}{:6.2f}{:6.2f}          {:>2s}{:2s}\"\n",
    "        self.atom_nbr = 1\n",
    "        self.res_nbr = 1\n",
    "        self.defaults = {\"alt_loc\": \"\",\n",
    "                         \"chain_id\": \"\",\n",
    "                         \"insertion_code\": \"\",\n",
    "                         \"occupancy\": 1,\n",
    "                         \"temp_factor\": 0,\n",
    "                         \"element_sym\": \"\",\n",
    "                         \"charge\": \"\"}\n",
    "        assert self.coords.shape[0] % self.atoms_per_res == 0, f\"Coords is not divisible by {atoms_per_res}. {self.coords.shape}\"\n",
    "        self.peptide_bond_full =   np.asarray([[0.519,  -2.968,   1.340],  # CA\n",
    "                                               [2.029,  -2.951,   1.374],  # C\n",
    "                                               [2.654,  -2.667,   2.392],  # O \n",
    "                                               [2.682,  -3.244,   0.300]]) # next-N\n",
    "        self.peptide_bond_mobile = np.asarray([[0.519,  -2.968,   1.340],  # CA\n",
    "                                               [2.029,  -2.951,   1.374],  # C\n",
    "                                               [2.682,  -3.244,   0.300]]) # next-N\n",
    "\n",
    "    def get_oxy_coords(self, ca, c, n):\n",
    "        target_coords = np.array([ca, c, n])\n",
    "        t = calcTransformation(self.peptide_bond_mobile, target_coords)\n",
    "        aligned_peptide_bond = t.apply(self.peptide_bond_full)\n",
    "        return aligned_peptide_bond[2]\n",
    "        \n",
    "    \n",
    "    def coord_generator(self):\n",
    "        coord_idx = 0\n",
    "        while coord_idx < self.coords.shape[0]:\n",
    "            if coord_idx + self.atoms_per_res + 1 < self.coords.shape[0]: \n",
    "                next_n = self.coords[coord_idx + self.atoms_per_res + 1]\n",
    "            else:\n",
    "                # TODO: Fix oxygen placement for final residue\n",
    "                next_n = self.coords[-1] +np.array([1.2, 0, 0])\n",
    "            yield self.coords[coord_idx:coord_idx + self.atoms_per_res], next_n\n",
    "            coord_idx += self.atoms_per_res\n",
    "            \n",
    "    def get_line_for_atom(self, res_name, atom_name, atom_coords, missing=False):\n",
    "        if missing:\n",
    "            occupancy = 0\n",
    "        else:\n",
    "            occupancy = self.defaults[\"occupancy\"]\n",
    "        return self.format_str.format(\"ATOM\",\n",
    "                                      self.atom_nbr,\n",
    "                                      \n",
    "                                      atom_name,\n",
    "                                      self.defaults[\"alt_loc\"],\n",
    "                                      res_name,\n",
    "                                      \n",
    "                                      self.defaults[\"chain_id\"],\n",
    "                                      self.res_nbr,\n",
    "                                      self.defaults[\"insertion_code\"],\n",
    "                                      \n",
    "                                      atom_coords[0],\n",
    "                                      atom_coords[1],\n",
    "                                      atom_coords[2],\n",
    "                                      occupancy,\n",
    "                                      self.defaults[\"temp_factor\"],\n",
    "                                      \n",
    "                                      atom_name[0],\n",
    "                                      self.defaults[\"charge\"])\n",
    "    \n",
    "    \n",
    "    def get_lines_for_residue(self, res_name, atom_names, coords, next_n):\n",
    "        residue_lines = []\n",
    "        for atom_name, atom_coord in zip(atom_names, coords):\n",
    "            if atom_name is \"PAD\" or np.isnan(atom_coord).sum() > 0:\n",
    "                continue\n",
    "#             if np.isnan(atom_coord).sum() > 0:\n",
    "#                 residue_lines.append(self.get_line_for_atom(res_name, atom_name, atom_coord, missing=True))\n",
    "#                 self.atom_nbr += 1\n",
    "#                 continue\n",
    "            residue_lines.append(self.get_line_for_atom(res_name, atom_name, atom_coord))\n",
    "            self.atom_nbr += 1\n",
    "        try:\n",
    "            oxy_coords = self.get_oxy_coords(coords[1], coords[2], next_n)\n",
    "            residue_lines.append(self.get_line_for_atom(res_name, \"O\", oxy_coords))\n",
    "            self.atom_nbr += 1\n",
    "        except ValueError:\n",
    "            pass\n",
    "        return residue_lines\n",
    "        \n",
    "    def get_lines_for_protein(self):\n",
    "        self.lines = []\n",
    "        self.res_nbr = 1\n",
    "        self.atom_nbr = 1\n",
    "        mapping_coords = zip(self.mapping, self.coord_generator())\n",
    "        prev_n = torch.tensor([0,0,-1])\n",
    "        for (res_name, atom_names), (res_coords, next_n) in mapping_coords:\n",
    "            self.lines.extend(self.get_lines_for_residue(res_name, atom_names, res_coords, next_n))\n",
    "            prev_n = res_coords[0]\n",
    "            self.res_nbr += 1\n",
    "        return self.lines\n",
    "    \n",
    "    def make_header(self, title):\n",
    "        return f\"REMARK  {title}\"\n",
    "    \n",
    "    def make_footer(self):\n",
    "        return \"TER\\nEND          \\n\"\n",
    "    \n",
    "    def save_pdb(self, path, title=\"test\"):\n",
    "        self.get_lines_for_protein()\n",
    "        self.lines = [self.make_header(title)] + self.lines + [self.make_footer()]\n",
    "        with open(path, \"w\") as outfile:\n",
    "            outfile.write(\"\\n\".join(self.lines))\n",
    "        print(f\"PDB written to {path}.\")\n",
    "        \n",
    "    def get_seq(self):\n",
    "        return \"\".join([THREE_TO_ONE_LETTER_MAP[m[0]] for m in self.mapping])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_map = get_13atom_mapping(one_letter_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"0924f_pred.pdb\"\n",
    "ttitle = title.replace(\"pred\", \"true\")\n",
    "pdbc = PDB_Creator(coords.squeeze(), cur_map)\n",
    "pdbc.save_pdb(title)\n",
    "pdbc = PDB_Creator(tgt_crds.squeeze(), cur_map)\n",
    "pdbc.save_pdb(ttitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align\n",
    "# p = parsePDB(title)\n",
    "# t = parsePDB(ttitle)\n",
    "# print(t.getCoords().shape, p.getCoords().shape)\n",
    "# tr = calcTransformation(t.getCoords(), p.getCoords())\n",
    "# t.setCoords(tr.apply(t.getCoords()))\n",
    "\n",
    "# writePDB(ttitle, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_a_prediction(title, data_iter):\n",
    "    src_seq, src_pos_enc, tgt_ang, tgt_pos_enc, tgt_crds, tgt_crds_enc =  next(data_iter)\n",
    "    tgt_ang_no_nan = tgt_ang.clone().detach()\n",
    "    tgt_ang_no_nan[torch.isnan(tgt_ang_no_nan)] = 0\n",
    "    pred = model(src_seq, src_pos_enc, tgt_ang_no_nan, tgt_pos_enc)\n",
    "    \n",
    "    # Calculate loss\n",
    "    d_loss, d_loss_normalized, r_loss = drmsd_loss_from_coords(pred, tgt_crds, src_seq, device,\n",
    "                                                                       return_rmsd=True)\n",
    "    m_loss = mse_over_angles(pred, tgt_ang).to('cpu')\n",
    "    \n",
    "    # Generate coords\n",
    "    pred = inverse_trig_transform(pred).squeeze()\n",
    "    src_seq = src_seq.squeeze()\n",
    "    coords = generate_coords(pred, pred.shape[0],src_seq, device)\n",
    "    \n",
    "    # Generate coord, atom_name mapping\n",
    "    one_letter_seq = onehot_to_seq(src_seq.squeeze().detach().numpy())\n",
    "    cur_map = get_13atom_mapping(one_letter_seq)\n",
    "    \n",
    "    # Make PDB Creator objects\n",
    "    pdb_pred = PDB_Creator(coords.squeeze(), cur_map)\n",
    "    pdb_true = PDB_Creator(tgt_crds.squeeze(), cur_map)\n",
    "    \n",
    "    # Save PDB files\n",
    "    pdb_pred.save_pdb(f\"{title}_pred.pdb\")\n",
    "    pdb_true.save_pdb(f\"{title}_true.pdb\")\n",
    "    \n",
    "    # Align PDB files\n",
    "    p = parsePDB(f\"{title}_pred.pdb\")\n",
    "    t = parsePDB(f\"{title}_true.pdb\")\n",
    "    tr = calcTransformation(p.getCoords()[:-1], t.getCoords())\n",
    "    p.setCoords(tr.apply(p.getCoords()))\n",
    "    \n",
    "    writePDB(f\"{title}_pred.pdb\", p)\n",
    "    \n",
    "    print(\"Constructed PDB files for\", title, \".\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_a_prediction(8, data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prody.apps.prody_apps.prody_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch_src2)",
   "language": "python",
   "name": "pytorch_src2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
